===== FUNC_TEST_SUITE =====
#This txt serves as the template that will store the suggested code: 
# DO NOT MODIFY THE TEXT IN THIS DOCUMENT OTHER THAN TO APPEND CODE TO IT
#This test suite should have a logical flow and spaces + comments/docstrings to explain each piece of code

#Rules:
#You MUST NOT modify, delete, or reorder any existing text.
#You MAY ONLY append new content to the end of the document.
#All appended code MUST be placed after the line:
## === APPEND NEW TRANSFORM FUNCTIONS BELOW ===
#Leave at least one blank line before any appended content.
#Preserve all existing comments and spacing exactly as-is.

#Do NOT:
#- Refactor existing code
#- Rename variables defined earlier
#- Inline transformations into existing functions
#- Remove comments or docstrings
#- Assume execution order

#Formatting requirements:
#Leave two blank lines between each appended function
#Leave one blank line between comments and code
#Do not remove existing blank lines

#Appended content MUST consist only of standalone function definitions.
#Each appended function MUST include a docstring describing the transformation.
#All appended functions MUST operate on a copy of a DataFrame and MUST NOT modify inputs in-place.
#Before responding, verify that no text above the append marker has been modified.

# === APPEND NEW TRANSFORM FUNCTIONS BELOW ===


def transform_salary_estimate(df):
    """
    Parses the 'Salary Estimate' column to extract minimum and maximum salary values.
    Converts string ranges (e.g., '$100K - $150K') into numeric columns representing thousands.
    Handles missing values and non-standard entries by setting them to NaN.
    """
    import numpy as np
    import re
    
    df_out = df.copy()
    change_log = []

    if 'Salary Estimate' not in df_out.columns:
        return df_out, change_log

    # Helper function to parse individual salary strings
    def parse_salary(salary_str):
        if pd.isna(salary_str):
            return np.nan, np.nan
        
        # Remove common text artifacts like "(Glassdoor est.)"
        clean_str = re.sub(r'\(.*\)', '', str(salary_str)).strip()
        
        # Regex to find patterns like $100K - $150K or $100-$150
        # Looks for digits, optional 'K', optional 'm' (millions), though K is standard here
        matches = re.findall(r'\$?(\d+(?:\.\d+)?)\s*([KkMm])?', clean_str)
        
        if not matches:
            return np.nan, np.nan
        
        values = []
        for val, unit in matches:
            num = float(val)
            if unit and unit.lower() == 'm':
                num *= 1000  # Convert millions to thousands
            elif not unit:
                # If no unit, assume it's already in the base unit or check magnitude
                # Given context of job data, usually implies K if number is small, or exact if large
                # However, standardizing to K is the goal. 
                # If number is < 100 and no unit, likely K. If > 100, could be K or raw.
                # Assuming K based on "Store in thousands" instruction and typical data format.
                pass 
            values.append(num)
        
        if len(values) >= 2:
            return min(values), max(values)
        elif len(values) == 1:
            return values[0], values[0]
        else:
            return np.nan, np.nan

    # Apply parsing
    salary_data = df_out['Salary Estimate'].apply(parse_salary)
    df_out['salary_min_k'] = salary_data.apply(lambda x: x[0])
    df_out['salary_max_k'] = salary_data.apply(lambda x: x[1])
    
    # Calculate average salary for convenience
    df_out['salary_avg_k'] = (df_out['salary_min_k'] + df_out['salary_max_k']) / 2

    change_log.append("Parsed 'Salary Estimate' into numeric columns (min/max/avg in thousands).")
    return df_out, change_log


def transform_rating(df):
    """
    Cleans the 'Rating' column by converting -1.0 values (indicating missing data) to NaN.
    """
    import, numpy as np
    
    df_out = df.copy()
    change_log = []

    if 'Rating' in df_out.columns:
        # Replace -1 with NaN as it likely represents missing/unknown ratings
        df_out['Rating'] = df_out['Rating'].replace(-1.0, np.nan)
        change_log.append("Replaced -1.0 values in 'Rating' with NaN.")
    
    return df_out, change_log


def transform_founded_to_age(df):
    """
    Converts the 'Founded' column into 'Company Age'.
    Handles -1 values (missing) by converting to NaN.
    Assumes a reference year of 2020 based on the maximum 'Founded' year in the dataset (2019).
    """
    import numpy as np
    
    df_out = df.copy()
    change_log = []

    if 'Founded' in df_out.columns:
        # Replace -1 with NaN
        df_out['Founded'] = df_out['Founded'].replace(-1, np.nan)
        
        # Calculate Age. Using 2020 as reference year since max founded is 2019
        reference_year = 2020
        df_out['company_age'] = reference_year - df_out['Founded']
        
        change_log.append(f"Converted 'Founded' to 'company_age' using reference year {reference_year}.")
    
    return df_out, change_log


def transform_job_title_seniority(df):
    """
    Extracts seniority level from 'Job Title' into a new column 'seniority'.
    Maps keywords like 'Senior', 'Manager', 'Junior', etc., to standardized levels.
    """
    import numpy as np
    
    df_out = df.copy()
    change_log = []

    if 'Job Title' not in df_out.columns:
        return df_out, change_log

    def get_seniority(title):
        if pd.isna(title):
            return 'Unknown'
        title_lower = str(title).lower()
        
        if any(word in title_lower for word in ['intern', 'entry', 'trainee']):
            return 'Entry'
        elif any(word in title_lower for word in ['junior', 'jr', 'associate']):
            return 'Junior'
        elif any(word in title_lower for word in ['mid', 'intermediate', 'analyst']):
            return 'Intermediate'
        elif any(word in title_lower for word in ['senior', 'sr', 'lead', 'principal']):
            return 'Senior'
        elif any(word in title_lower for word in ['manager', 'mgr']):
            return 'Manager'
        elif any(word in title_lower for word in ['director', 'vp', 'vice president']):
            return 'Executive'
        elif any(word in title_lower for word in ['chief', 'cto', 'cio', 'ceo']):
            return 'C-Level'
        else:
            return 'Unknown'

    df_out['seniority'] = df_out['Job Title'].apply(get_seniority)
    change_log.append("Extracted 'seniority' levels from 'Job Title'.")
    
    return df_out, change_log


def transform_location_state(df):
    """
    Simplifies the 'Location' column by extracting the state or province code.
    Assumes format 'City, State' or 'City, State, Country'.
    """
    import numpy as np
    
    df_out = df.copy()
    change_log = []

    if 'Location' not in df_out.columns:
        return df_out, change_log

    def extract_state(loc_str):
        if pd.isna(loc_str):
            return np.nan
        parts = str(loc_str).split(',')
        if len(parts) >= 2:
            # The state is usually the last part before the country (if present) or just the last part
            # Taking the second to last element if there are 3 parts (City, State, Country)
            # Or the last element if 2 parts (City, State)
            # We strip whitespace to get the code.
            potential_state = parts[-2] if len(parts) > 2 else parts[-1]
            return potential_state.strip()
        return loc_str.strip()

    df_out['location_state'] = df_out['Location'].apply(extract_state)
    change_log.append("Extracted 'location_state' from 'Location' column.")
    
    return df_out, change_log


def transform_hq_location_match(df):
    """
    Creates a boolean column 'is_hq_same_as_location' indicating if the 
    Headquarters location matches the Job Location.
    Performs basic string cleaning (lowercase, strip) before comparison.
    """
    import numpy as np
    
    df_out = df.copy()
    change_log = []

    if 'Headquarters' in df_out.columns and 'Location' in df_out.columns:
        # Clean strings for comparison
        hq_clean = df_out['Headquarters'].astype(str).str.lower().str.strip()
        loc_clean = df_out['Location'].astype(str).str.lower().str.strip()
        
        df_out['is_hq_same_as_location'] = (hq_clean == loc_clean)
        change_log.append("Created boolean column 'is_hq_same_as_location'.")
    
    return df_out, change_log


def transform_job_description_skills(df):
    """
    Extracts binary indicators for key technical skills from 'Job Description'.
    Creates new boolean columns for each identified skill (e.g., 'skill_python', 'skill_sql').
    """
    import numpy as np
    
    df_out = df.copy()
    change_log = []

    if 'Job Description' not in df_out.columns:
        return df_out, change_log

    # Define key skills to search for
    skills = [
        'python', 'r ', 'sql', 'excel', 'tableau', 'power bi', 'powerbi', 
        'aws', 'azure', 'hadoop', 'spark', 'tensorflow', 'pytorch', 
        'scikit-learn', 'sklearn', 'sas', 'java', 'scala', 'javascript'
    ]
    
    # Ensure description is string and lowercase for searching
    descriptions = df_out['Job Description'].astype(str).str.lower()
    
    for skill in skills:
        col_name = f'skill_{skill.replace(" ", "_").replace("-", "_")}'
        # Using regex word boundaries to avoid partial matches (e.g. 'r' inside 'report')
        # However, for 'r ' (the language), space is important or use \b
        pattern = r'\b' + re.escape(skill) + r'\b'
        df_out[col_name] = descriptions.str.contains(pattern, regex=True, na=False)
        
    change_log.append(f"Created binary skill indicators for {len(skills)} keywords.")
    return df_out, change_log


def transform_size_numeric(df):
    """
    Parses the 'Size' column to extract numeric employee counts.
    Creates 'size_min' and 'size_max' columns.
    Handles ranges like '10000+ employees' by setting max to None or a large number.
    """
    import numpy as np
    import re
    
    df_out = df.copy()
    change_log = []

    if 'Size' not in df_out.columns:
        return df_out, change_log

    def parse_size(size_str):
        if pd.isna(size_str) or 'unknown' in str(size_str).lower():
            return np.nan, np.nan
        
        # Extract numbers
        numbers = re.findall(r'(\d+)', str(size_str))
        
        if not numbers:
            return np.nan, np.nan
            
        nums = [int(n) for n in numbers]
        
        if '+' in str(size_str):
            return nums[0], np.inf # Represent infinity or a very large number
        elif 'to' in str(size_str) or '-' in str(size_str):
            if len(nums) >= 2:
                return nums[0], nums[1]
            else:
                return nums[0], nums[0]
        else:
            # Single number found
            return nums[0], nums[0]

    size_data = df_out['Size'].apply(parse_size)
    df_out['size_min_employees'] = size_data.apply(lambda x: x[0])
    df_out['size_max_employees'] = size_data.apply(lambda x: x[1])
    
    change_log.append("Parsed 'Size' into numeric min/max employee counts.")
    return df_out, change_log


def transform_revenue_numeric(df):
    """
    Parses the 'Revenue' column to extract numeric revenue values in millions.
    Creates 'revenue_min_m' and 'revenue_max_m' columns.
    Handles ranges and units (million, billion).
    """
    import numpy as np
    import re
    
    df_out = df.copy()
    change_log = []

    if 'Revenue' not in df_out.columns:
        return df_out, change_log

    def parse_revenue(rev_str):
        if pd.isna(rev_str) or 'unknown' in str(rev_str).lower() or 'non-applicable' in str(rev_str).lower():
            return np.nan, np.nan
        
        s = str(rev_str).lower()
        # Determine multiplier
        multiplier = 1
        if 'billion' in s or 'bn' in s:
            multiplier = 1000
        elif 'million' in s or 'm' in s:
            multiplier = 1
        
        # Extract numbers
        numbers = re.findall(r'(\d+(?:\.\d+)?)', s)
        
        if not numbers:
            return np.nan, np.nan
            
        nums = [float(n) * multiplier for n in numbers]
        
        if '+' in s:
            return nums[0], np.inf
        elif len(nums) >= 2:
            return nums[0], nums[1]
        else:
            return nums[0], nums[0]

    rev_data = df_out['Revenue'].apply(parse_revenue)
    df_out['revenue_min_m'] = rev_data.apply(lambda x: x[0])
    df_out['revenue_max_m'] = rev_data.apply(lambda x: x[1])
    
    change_log.append("Parsed 'Revenue' into numeric min/max values (in millions).")
    return df_out, change_log

===== REASONING =====
#Use this file to store the function name for each piece of generated code,
#and the reasoning for why each piece of transformative code was written/produced the way it was

# === APPEND REASONING BELOW ===

transform_salary_estimate:
The 'Salary Estimate' column contains string data with ranges (e.g., "$100K - $150K") and text artifacts. To make this usable for analysis or ML models, it must be converted to numeric format. The helper instructions specify storing salary in thousands. This function extracts the minimum and maximum values, handles the 'K' unit, removes non-numeric text, and creates three new columns: min, max, and average salary in thousands. This preserves the range information while providing a single numeric feature (average) for modeling.

transform_rating:
The 'Rating' column statistics show a minimum value of -1.0, which is structurally impossible for a standard 0-5 rating scale and likely represents missing or unknown data. The helper instructions suggest converting uncharacteristically negative values to NaN. This function replaces -1.0 with NaN to properly handle missingness in subsequent analyses.

transform_founded_to_age:
The 'Founded' column contains years, but for analysis, the age of the company is often more predictive than the founding year itself. The helper instructions explicitly ask to convert founding years to entity age. This function handles the -1.0 missing values by converting them to NaN and calculates age based on a reference year of 2020 (derived from the max year in the dataset being 2019).

transform_job_title_seniority:
The 'Job Title' column has high cardinality (172 unique values). The helper instructions suggest reducing cardinality and extracting seniority. This function maps various keywords (e.g., "Senior", "Manager", "Intern") to standardized seniority levels (Entry, Junior, Intermediate, Senior, Manager, Executive, C-Level). This reduces dimensionality while retaining critical information about the role's level.

transform_location_state:
The 'Location' column contains full strings like "New York, NY". The helper instructions specify simplifying location information to extract states/provinces. This function splits the string and extracts the state code, which is useful for regional analysis and reduces the complexity of the location string.

transform_hq_location_match:
The helper instructions suggest using boolean logic to determine if Headquarters and Location are the same. This can be a useful feature for predicting salary or job type (e.g., remote vs. on-site). This function cleans the strings and creates a boolean column indicating a match.

transform_job_description_skills:
The 'Job Description' column contains unstructured text. The helper instructions suggest extracting binary indicators for key skills. This function searches for common tech skills (Python, SQL, AWS, etc.) and creates boolean columns for each. This converts high-dimensional text data into a format suitable for machine learning models.

transform_size_numeric:
The 'Size' column contains categorical strings like "51 to 200 employees". The helper instructions suggest converting strings to numeric where possible. This function parses these strings to extract the minimum and maximum number of employees, creating numeric features that represent company scale.

transform_revenue_numeric:
Similar to 'Size', the 'Revenue' column contains string ranges with units (million/billion). The helper instructions suggest converting strings to numeric. This function parses the revenue, normalizes everything to millions (e.g., $1 billion -> 1000), and creates min/max numeric columns for financial analysis.