===== FUNC_TEST_SUITE =====
#This txt serves as the template that will store the suggested code: 
# DO NOT MODIFY THE TEXT IN THIS DOCUMENT OTHER THAN TO APPEND CODE TO IT
#This test suite should have a logical flow and spaces + comments/docstrings to explain each piece of code

#Rules:
#You MUST NOT modify, delete, or reorder any existing text.
#You MAY ONLY append new content to the end of the document.
#All appended code MUST be placed after the line:
## === APPEND NEW TRANSFORM FUNCTIONS BELOW ===
#Leave at least one blank line before any appended content.
#Preserve all existing comments and spacing exactly as-is.

#Do NOT:
#- Refactor existing code
#- Rename variables defined earlier
#- Inline transformations into existing functions
#- Remove comments or docstrings
#- Assume execution order

#Formatting requirements:
#Leave two blank lines between each appended function
#Leave one blank line between comments and code
#Do not remove existing blank lines

#Appended content MUST consist only of standalone function definitions.
#Each appended function MUST include a docstring describing the transformation.
#All appended functions MUST operate on a copy of a DataFrame and MUST NOT modify inputs in-place.
#Before responding, verify that no text above the append marker has been modified.

# === APPEND NEW TRANSFORM FUNCTIONS BELOW ===


def transform_salary_estimate(df):
    """
    Parses the 'Salary Estimate' column to extract numeric salary values.
    Converts 'K' (thousands) to full numeric values, then stores them in thousands 
    for readability as per ML standards.
    Creates 'Salary_Min_K', 'Salary_Max_K', and 'Salary_Avg_K' columns.
    """
    df_out = df.copy()
    change_log = []

    if 'Salary Estimate' in df_out.columns:
        # Extract numeric values using regex. Handles formats like "$56K-$91K" or "$56,000-$91,000"
        # We look for digits, optionally followed by K or commas.
        salary_pattern = r'\$?([\d,]+)([Kk]?)\s*-\s*\$?([\d,]+)([Kk]?)'
        
        extracted = df_out['Salary Estimate'].str.extract(salary_pattern)
        
        def parse_salary(val_str, unit_str):
            if pd.isna(val_str):
                return np.nan
            val = float(val_str.replace(',', ''))
            if unit_str and unit_str.lower() == 'k':
                # If unit is K, value is already in thousands (e.g. 56K -> 56)
                return val
            else:
                # If no unit, assume it's raw number, convert to thousands
                return val / 1000.0

        df_out['Salary_Min_K'] = extracted.apply(lambda row: parse_salary(row[0], row[1]), axis=1)
        df_out['Salary_Max_K'] = extracted.apply(lambda row: parse_salary(row[2], row[3]), axis=1)
        
        # Calculate average
        df_out['Salary_Avg_K'] = (df_out['Salary_Min_K'] + df_out['Salary_Max_K']) / 2
        
        change_log.append("Parsed 'Salary Estimate' into numeric columns (Min, Max, Avg in K).")
    else:
        change_log.append("Column 'Salary Estimate' not found.")

    return df_out, change_log


def extract_job_seniority(df):
    """
    Extracts seniority level from the 'Job Title' column.
    Maps keywords to 'Senior', 'Mid', 'Junior', 'Entry', or 'Executive'.
    Reduces cardinality of job titles for analysis.
    """
    df_out = df.copy()
    change_log = []

    if 'Job Title' in df_out.columns:
        def get_seniority(title):
            if pd.isna(title):
                return 'Unknown'
            title_lower = str(title).lower()
            
            if any(word in title_lower for word in ['intern', 'entry', 'trainee']):
                return 'Entry'
            elif any(word in title_lower for word in ['junior', 'jr', 'associate']):
                return 'Junior'
            elif any(word in title_lower for word in ['senior', 'sr', 'lead', 'principal', 'staff']):
                return 'Senior'
            elif any(word in title_lower for word in ['manager', 'director', 'vp', 'vice president', 'head', 'chief']):
                return 'Executive'
            else:
                return 'Mid'
        
        df_out['Seniority'] = df_out['Job Title'].apply(get_seniority)
        change_log.append("Extracted 'Seniority' from 'Job Title'.")
    else:
        change_log.append("Column 'Job Title' not found.")

    return df_out, change_log


def calculate_company_age(df):
    """
    Converts the 'Founded' year into 'Company_Age'.
    Handles -1 (missing/unknown) values by converting them to NaN.
    """
    df_out = df.copy()
    change_log = []

    if 'Founded' in df_out.columns:
        current_year = pd.Timestamp.now().year
        
        # Replace -1 with NaN to calculate age correctly (or result in NaN)
        founded_clean = df_out['Founded'].replace(-1, np.nan)
        
        df_out['Company_Age'] = current_year - founded_clean
        
        # If Founded was -1, Age is NaN. If it was a valid year, Age is int.
        df_out['Company_Age'] = df_out['Company_Age'].astype('Int64') # Nullable integer
        
        change_log.append(f"Calculated 'Company_Age' from 'Founded' using current year {current_year}.")
    else:
        change_log.append("Column 'Founded' not found.")

    return df_out, change_log


def simplify_location(df):
    """
    Simplifies the 'Location' column by extracting the state or country code.
    Assumes format "City, State" or "City, Country".
    """
    df_out = df.copy()
    change_log = []

    if 'Location' in df_out.columns:
        def extract_state(loc):
            if pd.isna(loc):
                return np.nan
            parts = str(loc).split(',')
            if len(parts) > 1:
                return parts[-1].strip()
            return loc # Return as is if no comma

        df_out['Location_Simple'] = df_out['Location'].apply(extract_state)
        change_log.append("Simplified 'Location' to 'Location_Simple' (State/Country).")
    else:
        change_log.append("Column 'Location' not found.")

    return df_out, change_log


def compare_hq_location(df):
    """
    Creates a boolean column 'Is_HQ_Location' indicating if the job location 
    matches the company headquarters.
    """
    df_out = df.copy()
    change_log = []

    if 'Location' in df_out.columns and 'Headquarters' in df_out.columns:
        # Simple string comparison. 
        # Note: This might be strict if formatting differs (e.g. "New York, NY" vs "New York").
        # We strip whitespace to be safe.
        df_out['Is_HQ_Location'] = (df_out['Location'].str.strip() == df_out['Headquarters'].str.strip())
        change_log.append("Created 'Is_HQ_Location' boolean comparing 'Location' and 'Headquarters'.")
    else:
        change_log.append("Columns 'Location' or 'Headquarters' not found.")

    return df_out, change_log


def extract_job_skills(df):
    """
    Extracts binary indicators for key skills from the 'Job Description' column.
    Checks for common data science tools: Python, R, SQL, Excel, Tableau, Power BI, AWS, Spark, Hadoop.
    """
    df_out = df.copy()
    change_log = []

    if 'Job Description' in df_out.columns:
        skills = ['python', 'r ', 'sql', 'excel', 'tableau', 'power bi', 'aws', 'spark', 'hadoop', 'sas', 'java']
        
        # Ensure description is string and lowercase for searching
        descriptions = df_out['Job Description'].astype(str).str.lower()
        
        for skill in skills:
            col_name = f'Skill_{skill.replace(" ", "_").capitalize()}'
            # Using regex word boundaries to avoid partial matches (e.g. "r" inside "report")
            # Special handling for 'R' to avoid matching single letters everywhere, usually ' R ' or ' R,' is safer
            if skill == 'r ':
                df_out[col_name] = descriptions.str.contains(r'\br\b', na=False)
            else:
                df_out[col_name] = descriptions.str.contains(skill, na=False)
                
        change_log.append(f"Extracted skill indicators for: {', '.join(skills)}.")
    else:
        change_log.append("Column 'Job Description' not found.")

    return df_out, change_log


def parse_revenue(df):
    """
    Parses the 'Revenue' column into numeric values in Millions.
    Handles ranges (e.g., "$10 to $25 million") and unknown values.
    Creates 'Revenue_Min_M' and 'Revenue_Max_M'.
    """
    df_out = df.copy()
    change_log = []

    if 'Revenue' in df_out.columns:
        # Normalize text: lower case, remove dollar signs, replace 'million'/'billion'
        rev_series = df_out['Revenue'].astype(str).str.lower()
        
        # Helper to parse a single value string
        def parse_value(val_str):
            if pd.isna(val_str) or 'unknown' in val_str or 'non-applicable' in val_str:
                return np.nan
            
            val_str = val_str.replace('$', '').replace(',', '').strip()
            
            # Extract number
            num_match = re.search(r'([\d.]+)', val_str)
            if not num_match:
                return np.nan
            num = float(num_match.group(1))
            
            # Determine multiplier
            if 'billion' in val_str or 'bn' in val_str:
                return num * 1000
            elif 'million' in val_str or 'm' in val_str:
                return num
            else:
                # Assume millions if no unit, or handle as raw if context suggests (usually millions in this dataset)
                return num 

        # Extract ranges using regex for "X to Y" or "X - Y"
        range_pattern = r'([\d.$]+)\s*(?:to|-)\s*([\d.$]+)\s*(million|billion|m|bn)?'
        
        # We need to handle the parsing carefully because the unit applies to the whole range usually
        # Strategy: Extract the whole string, find the two numbers, find the unit.
        
        parsed_data = []
        for text in rev_series:
            if 'unknown' in text or 'non-applicable' in text:
                parsed_data.append((np.nan, np.nan))
                continue
                
            # Find unit
            multiplier = 1
            if 'billion' in text or 'bn' in text:
                multiplier = 1000
            elif 'million' in text or 'm' in text:
                multiplier = 1
            
            # Find numbers
            numbers = re.findall(r'([\d.]+)', text.replace('$', '').replace(',', ''))
            if len(numbers) >= 2:
                min_val = float(numbers[0]) * multiplier
                max_val = float(numbers[1]) * multiplier
            elif len(numbers) == 1:
                # Single number or "Less than X"
                val = float(numbers[0]) * multiplier
                min_val = val
                max_val = val
            else:
                min_val = np.nan
                max_val = np.nan
                
            parsed_data.append((min_val, max_val))
            
        result_df = pd.DataFrame(parsed_data, columns=['Revenue_Min_M', 'Revenue_Max_M'])
        df_out = pd.concat([df_out, result_df], axis=1)
        
        change_log.append("Parsed 'Revenue' into numeric columns (Min, Max in Millions).")
    else:
        change_log.append("Column 'Revenue' not found.")

    return df_out, change_log


def parse_size(df):
    """
    Parses the 'Size' column into numeric employee counts.
    Handles ranges (e.g., "51 to 200 employees") and open-ended ranges (e.g., "10000+ employees").
    Creates 'Size_Min' and 'Size_Max'.
    """
    df_out = df.copy()
    change_log = []

    if 'Size' in df_out.columns:
        size_series = df_out['Size'].astype(str).str.lower()
        
        parsed_data = []
        for text in size_series:
            if 'unknown' in text or pd.isna(text):
                parsed_data.append((np.nan, np.nan))
                continue
            
            # Remove non-numeric except + and -
            # Extract numbers
            numbers = re.findall(r'([\d]+)', text)
            
            if len(numbers) >= 2:
                min_val = int(numbers[0])
                max_val = int(numbers[1])
            elif len(numbers) == 1:
                val = int(numbers[0])
                if '+' in text:
                    min_val = val
                    max_val = val # Or could be a large constant, but keeping equal is safer for analysis
                else:
                    min_val = val
                    max_val = val
            else:
                min_val = np.nan
                max_val = np.nan
            
            parsed_data.append((min_val, max_val))
            
        result_df = pd.DataFrame(parsed_data, columns=['Size_Min', 'Size_Max'])
        df_out = pd.concat([df_out, result_df], axis=1)
        
        change_log.append("Parsed 'Size' into numeric columns (Min, Max employees).")
    else:
        change_log.append("Column 'Size' not found.")

    return df_out, change_log

===== REASONING =====
#Use this file to store the function name for each piece of generated code,
#and the reasoning for why each piece of transformative code was written/produced the way it was

# === APPEND REASONING BELOW ===

1. transform_salary_estimate:
   - Reasoning: The 'Salary Estimate' column is currently a string object with high cardinality (30 unique values) and contains ranges (e.g., "$56K-$91K"). To make this usable for ML models or statistical analysis, it must be converted to numeric. The helper_reg specifies that 'K' means thousand and salaries should be stored in thousands for readability. This function extracts the minimum and maximum values, handles the 'K' unit, and calculates an average, creating three new numeric columns.

2. extract_job_seniority:
   - Reasoning: The 'Job Title' column has high cardinality (172 unique values). The helper_reg suggests reducing cardinality and extracting seniority attributes. By mapping keywords (e.g., "Senior", "Junior", "Manager") to standardized categories, we reduce the dimensionality significantly while retaining critical information about the role level, which is often a strong predictor for salary.

3. calculate_company_age:
   - Reasoning: The 'Founded' column contains years, but for analysis, the "age" of a company is often more relevant than the founding year itself. The helper_reg explicitly requests converting founding years to entity age. This function calculates the difference between the current year and the 'Founded' year. It also handles the -1 values (likely missing data) by converting them to NaN to prevent skewing the age calculation.

4. simplify_location:
   - Reasoning: The 'Location' column contains full strings like "New York, NY". The helper_reg advises simplifying location information to states/provinces or countries. Extracting just the state (e.g., "NY") reduces cardinality (from 207 unique values) and allows for easier regional grouping and analysis (e.g., cost of living by state).

5. compare_hq_location:
   - Reasoning: The helper_reg suggests using boolean logic to determine if two elements are the same if useful. Comparing 'Location' and 'Headquarters' can indicate if a role is based at the company's main campus, which might correlate with company culture or compensation structure. This creates a binary feature useful for classification.

6. extract_job_skills:
   - Reasoning: The 'Job Description' is a text field. The helper_reg requests extracting binary indicators for key skills (e.g., Python, Excel) for ML training. This function scans the text for a curated list of common data science tools and creates one-hot encoded columns, transforming unstructured text into structured features.

7. parse_revenue:
   - Reasoning: The 'Revenue' column is a string object with ranges (e.g., "$10 to $25 million"). Similar to salary, this needs to be numeric for analysis. The helper_reg asks to convert string ranges to numeric. This function parses the text, handles "million" vs "billion" units (standardizing to Millions), and creates min/max numeric columns.

8. parse_size:
   - Reasoning: The 'Size' column describes employee count as strings (e.g., "51 to 200 employees"). To analyze the impact of company size on salary or rating, this needs to be numeric. This function extracts the numeric bounds from the string ranges, handling special cases like "10000+" (where min equals max) and "Unknown".