{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3b5333",
   "metadata": {},
   "source": [
    "This notebook with call the necessary functions to pass a data frame to an LLM and have it make suggestions for the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c3110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library import for loading df\n",
    "from llm_data_checker import read_df\n",
    "\n",
    "\n",
    "#incase of error with file cache uncomment below command \n",
    "#importlib.reload(llm_data_checker)\n",
    "\n",
    "data = read_df(\"data_test/Uncleaned_DS_jobs.csv\")\n",
    "\n",
    "\n",
    "#run  source .venv/bin/activate to activate virtual environment first\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af013ec7",
   "metadata": {},
   "source": [
    "Run this cell to check basic stats of df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a78443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_data_checker import df_checker_v2\n",
    "\n",
    "check_data = df_checker_v2(data)\n",
    "\n",
    "\n",
    "#write out the stats to external directory\n",
    "with open(\"stats_output/stats.txt\", \"w\") as f:\n",
    "    for section_name, section_value in check_data.items():\n",
    "        #write a header for this section\n",
    "        f.write(f\"===== {section_name} =====\\n\")\n",
    "        \n",
    "        #write the actual data\n",
    "        f.write(str(section_value))\n",
    "        \n",
    "        #add spacing\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e1bc84",
   "metadata": {},
   "source": [
    "Anonymiser for data exists within the df_checker function. It takes stats, shapes and patterns to inform the LLM for data cleaning suggestions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22df4487",
   "metadata": {},
   "source": [
    "Prompt builder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918c89ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10554\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "\n",
    "#point to directoy holding prompt files\n",
    "framework_txt = Path(\"frameworks\")\n",
    "\n",
    "#points to the directory holding the df stats \n",
    "stats_txt = Path(\"stats_output\" )\n",
    "\n",
    "#read the actual files themselves\n",
    "system_template = (framework_txt / \"prompt.txt\").read_text()\n",
    "\n",
    "\n",
    "func_test_suite = (framework_txt / \"func_test_suite.txt\").read_text()\n",
    "function_format = (framework_txt / \"function_format.txt\").read_text()\n",
    "stats = (stats_txt / \"stats.txt\").read_text()\n",
    "reasoning = (framework_txt / \"reasoning.txt\").read_text()\n",
    "helper_reg = (framework_txt / \"helper_reg.txt\").read_text()\n",
    "\n",
    "\n",
    "#build the actual prompt \n",
    "prompt = system_template.format(\n",
    "    func_test_suite=func_test_suite,\n",
    "    function_format=function_format,\n",
    "    stats=stats,\n",
    "    reasoning=reasoning,\n",
    "    helper_reg=helper_reg\n",
    ")\n",
    "\n",
    "#write out the stats to external directory\n",
    "with open(\"final_prompt/combined_prompt.txt\", \"w\") as f:\n",
    "    f.write(prompt)\n",
    "\n",
    "combined_prompt = Path(\"final_prompt/combined_prompt.txt\").read_text()\n",
    "print(len(combined_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5950967f",
   "metadata": {},
   "source": [
    "API call out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad20536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerebras.cloud.sdk import Cerebras\n",
    "import os\n",
    "\n",
    "client = Cerebras(api_key=os.environ[\"CEREBRAS_API_KEY\"])\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"zai-glm-4.7\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": combined_prompt},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"proceed\"\n",
    "        },\n",
    "    ],\n",
    "    max_completion_tokens=8192,  #increase for complex analysis\n",
    "    temperature=0.0,\n",
    "    top_p=0.95,  #\n",
    "    frequency_penalty=0.0,  #reduce repetition\n",
    "    presence_penalty=0.0,   #encourages completeness\n",
    ")\n",
    "\n",
    "output = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535e3d7",
   "metadata": {},
   "source": [
    "Write out results from LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0627aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16508"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "out_path = Path(\"llm_cleaning/llm_output.txt\")\n",
    "out_path.write_text(output, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b3352",
   "metadata": {},
   "source": [
    "Chop .txt results from cleaning operation and feed results into LLM for suggestions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "306c0076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"llm_cleaning/llm_output.txt\")\n",
    "text = path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "marker = \"# === APPEND NEW TRANSFORM FUNCTIONS BELOW ===\"\n",
    "\n",
    "if marker in text:\n",
    "    chopped = text.split(marker, 1)[1]  # keep everything AFTER\n",
    "else:\n",
    "    chopped = text  # fallback if marker not found\n",
    "\n",
    "output_path = Path(\"temp/llm_output_chopped.txt\")\n",
    "output_path.write_text(chopped.strip(), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977978c3",
   "metadata": {},
   "source": [
    "Read in results from LLM for suggestion generation if desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68842849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3265"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cerebras.cloud.sdk import Cerebras\n",
    "import os\n",
    "\n",
    "\n",
    "#point to directoy holding prompt files\n",
    "framework_txt = Path(\"frameworks\")\n",
    "\n",
    "#points to the directory holding the df stats \n",
    "stats_txt = Path(\"stats_output\" )\n",
    "\n",
    "#read the actual files themselves\n",
    "system_template = (framework_txt / \"suggestions_format.txt\").read_text()\n",
    "\n",
    "stats = (stats_txt / \"stats.txt\").read_text()\n",
    "chopped_output = Path(\"temp/llm_output_chopped.txt\").read_text(encoding=\"utf-8\" )\n",
    "\n",
    "#build the actual prompt \n",
    "prompt = system_template.format(\n",
    "    stats=stats,\n",
    "    llm_output_chopped=chopped_output\n",
    ")\n",
    "\n",
    "client = Cerebras(api_key=os.environ[\"CEREBRAS_API_KEY\"])\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"zai-glm-4.7\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"proceed\"\n",
    "        },\n",
    "    ],\n",
    "    max_completion_tokens=8192,  #increase for complex analysis\n",
    "    temperature=0.0,\n",
    "    top_p=0.95,  #\n",
    "    frequency_penalty=0.0,  #reduce repetition\n",
    "    presence_penalty=0.0,   #encourages completeness\n",
    ")\n",
    "\n",
    "output = completion.choices[0].message.content\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "out_path = Path(\"llm_suggestions/llm_suggestions.txt\")\n",
    "out_path.write_text(output, encoding=\"utf-8\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
