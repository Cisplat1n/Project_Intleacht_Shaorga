{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3b5333",
   "metadata": {},
   "source": [
    "This notebook with call the necessary functions to pass a data frame to an LLM and have it make suggestions for the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c3110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library import for loading df\n",
    "from llm_data_checker import read_df\n",
    "\n",
    "\n",
    "#incase of error with file cache uncomment below command \n",
    "#importlib.reload(llm_data_checker)\n",
    "\n",
    "data = read_df(\"data_test/Uncleaned_DS_jobs.csv\")\n",
    "\n",
    "\n",
    "#run  source .venv/bin/activate to activate virtual environment first\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af013ec7",
   "metadata": {},
   "source": [
    "Run this cell to check basic stats of df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a78443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_data_checker import df_checker\n",
    "\n",
    "check_data = df_checker(data)\n",
    "\n",
    "\n",
    "#write out the stats to external directory\n",
    "with open(\"stats_output/stats.txt\", \"w\") as f:\n",
    "    for section_name, section_value in check_data.items():\n",
    "        # Write a header for this section\n",
    "        f.write(f\"===== {section_name} =====\\n\")\n",
    "        \n",
    "        # Write the actual data\n",
    "        f.write(str(section_value))\n",
    "        \n",
    "        # Add spacing\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e1bc84",
   "metadata": {},
   "source": [
    "Anonymiser: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc03340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22df4487",
   "metadata": {},
   "source": [
    "Prompt builder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c89ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "\n",
    "#point to directoy holding prompt files\n",
    "framework_txt = Path(\"frameworks\")\n",
    "\n",
    "#points to the directory holding the df stats \n",
    "stats_txt = Path(\"stats_output\" )\n",
    "\n",
    "#read the actual files themselves\n",
    "system_template = (framework_txt / \"prompt.txt\").read_text()\n",
    "\n",
    "\n",
    "func_test_suite = (framework_txt / \"func_test_suite.txt\").read_text()\n",
    "function_format = (framework_txt / \"function_format.txt\").read_text()\n",
    "stats = (stats_txt / \"stats.txt\").read_text()\n",
    "reasoning = (framework_txt / \"reasoning.txt\").read_text()\n",
    "\n",
    "\n",
    "#build the actual prompt \n",
    "\n",
    "prompt = system_template.format(\n",
    "    func_test_suite=func_test_suite,\n",
    "    function_format=function_format,\n",
    "    stats=stats,\n",
    "    reasoning=reasoning,\n",
    ")\n",
    "\n",
    "#write out the stats to external directory\n",
    "with open(\"final_prompt/combined_prompt.txt\", \"w\") as f:\n",
    "    f.write(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0730e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_prompt = Path(\"final_prompt/combined_prompt.txt\").read_text()\n",
    "print(len(combined_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5950967f",
   "metadata": {},
   "source": [
    "API call out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ad20536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func_context chars: 1327\n",
      "function_format chars: 176\n",
      "stats chars: 2299\n",
      "reasoning chars: 206\n"
     ]
    }
   ],
   "source": [
    "from cerebras.cloud.sdk import Cerebras\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "client = Cerebras(api_key=os.environ[\"CEREBRAS_API_KEY\"])\n",
    "\n",
    "# --- Load files ---\n",
    "func_test_suite = Path(\"frameworks/func_test_suite.txt\").read_text()\n",
    "function_format = Path(\"frameworks/function_format.txt\").read_text()\n",
    "stats = Path(\"stats_output/stats.txt\").read_text()\n",
    "reasoning = Path(\"frameworks/reasoning.txt\").read_text()\n",
    "\n",
    "# --- Trim func_test_suite ---\n",
    "marker = \"=== APPEND NEW TRANSFORM FUNCTIONS BELOW ===\"\n",
    "head, _, tail = func_test_suite.partition(marker)\n",
    "tail_lines = tail.splitlines()[-200:]\n",
    "\n",
    "func_context = (\n",
    "    head\n",
    "    + marker\n",
    "    + \"\\n\"\n",
    "    + \"\\n\".join(tail_lines)\n",
    ")\n",
    "\n",
    "# --- System prompt (rules only) ---\n",
    "system_prompt = f\"\"\"\n",
    "You are an expert Python developer.\n",
    "\n",
    "You are operating in APPEND-ONLY mode.\n",
    "\n",
    "Rules:\n",
    "- You MUST NOT modify text above the append marker.\n",
    "- You MAY ONLY append new standalone functions.\n",
    "- Preserve spacing, comments, and formatting.\n",
    "- Each function must operate on a copy of a DataFrame.\n",
    "- Leave two blank lines between appended functions.\n",
    "\n",
    "The contents of func_test_suite.txt are authoritative.\n",
    "\"\"\"\n",
    "\n",
    "# --- User prompt (variable content) ---\n",
    "user_prompt = f\"\"\"\n",
    "Here is the current state of the test suite:\n",
    "\n",
    "{func_context}\n",
    "\n",
    "You MUST follow this function format:\n",
    "\n",
    "{function_format}\n",
    "\n",
    "Here are the dataset statistics:\n",
    "\n",
    "{stats}\n",
    "\n",
    "Use reasoning guidance from:\n",
    "\n",
    "{reasoning}\n",
    "\n",
    "Proceed.\n",
    "\"\"\"\n",
    "\n",
    "print(\"func_context chars:\", len(func_context))\n",
    "print(\"function_format chars:\", len(function_format))\n",
    "print(\"stats chars:\", len(stats))\n",
    "print(\"reasoning chars:\", len(reasoning))\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    "    max_completion_tokens=1024,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "output = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535e3d7",
   "metadata": {},
   "source": [
    "Write out results from LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0627aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1784"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "out_path = Path(\"llm_suggestions/llm_output.txt\")\n",
    "out_path.write_text(output, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
