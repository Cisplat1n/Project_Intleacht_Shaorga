===== FUNC_TEST_SUITE =====
#This txt serves as the template that will store the suggested code: 
# DO NOT MODIFY THE TEXT IN THIS DOCUMENT OTHER THAN TO APPEND CODE TO IT
#This test suite should have a logical flow and spaces + comments/docstrings to explain each piece of code

#Rules:
#You MUST NOT modify, delete, or reorder any existing text.
#You MAY ONLY append new content to the end of the document.
#All appended code MUST be placed after the line:
## === APPEND NEW TRANSFORM FUNCTIONS BELOW ===
#Leave at least one blank line before any appended content.
#Preserve all existing comments and spacing exactly as-is.

#Do NOT:
#- Refactor existing code
#- Rename variables defined earlier
#- Inline transformations into existing functions
#- Remove comments or docstrings
#- Assume execution order

#Formatting requirements:
#Leave two blank lines between each appended function.
#Leave one blank line between comments and code.
#Do not remove existing blank lines.

#Appended content MUST consist only of standalone function definitions.
#Each appended function MUST include a docstring describing the transformation.
#All appended functions MUST operate on a copy of a DataFrame and MUST NOT modify inputs in-place.
#Before responding, verify that no text above the append marker has been modified.

# === APPEND NEW TRANSFORM FUNCTIONS BELOW ===


def clean_company_name(df):
    """
    Removes embedded newline characters and ratings from the Company Name column.
    
    The data indicates that Company Name often contains a rating appended on a new line
    (e.g., "Company Name\\n3.5"). This transformation splits the string and retains only
    the first part to isolate the actual company name.
    """
    df_out = df.copy()
    change_log = []

    if 'Company Name' in df_out.columns:
        original_count = df_out['Company Name'].count()
        # Split by newline and take the first part
        df_out['Company Name'] = df_out['Company Name'].apply(lambda x: x.split('\n')[0] if isinstance(x, str) else x)
        new_count = df_out['Company Name'].count()
        change_log.append(f"Cleaned 'Company Name': Removed embedded newlines/ratings. Rows affected: {original_count}")
    else:
        change_log.append("Column 'Company Name' not found. No transformation applied.")

    return df_out, change_log


def clean_job_description(df):
    """
    Removes embedded newline characters from the Job Description column.
    
    The analysis shows 94% of Job Descriptions contain newlines, suggesting formatting
    artifacts. This replaces newlines with spaces to create a single continuous string
    per description, which is better for text processing and skill extraction.
    """
    df_out = df.copy()
    change_log = []

    if 'Job Description' in df_out.columns:
        # Replace newlines with spaces
        df_out['Job Description'] = df_out['Job Description'].str.replace('\n', ' ', regex=False)
        change_log.append("Cleaned 'Job Description': Replaced newlines with spaces.")
    else:
        change_log.append("Column 'Job Description' not found. No transformation applied.")

    return df_out, change_log


def parse_salary_estimate(df):
    """
    Parses the 'Salary Estimate' column to extract minimum, maximum, and average salary.
    
    The data contains ranges like '$80K-$120K' (Glassdoor est.). This function extracts
    the numeric values, handles the 'K' suffix (thousands), and stores the results in
    thousands (e.g., 80 instead of 80000) per ML dataset standards.
    """
    df_out = df.copy()
    change_log = []

    if 'Salary Estimate' in df_out.columns:
        import re
        
        def extract_salary(s):
            if not isinstance(s, str):
                return None, None, None
            
            # Find all patterns like $80K or $120
            matches = re.findall(r'\$?(\d+)K?', s)
            if len(matches) >= 2:
                min_sal = int(matches[0])
                max_sal = int(matches[1])
                
                # Check if 'K' was present to determine scale, though usually consistent in range
                # Assuming standard format where K implies thousands
                if 'K' in s.upper():
                    min_sal *= 1000
                    max_sal *= 1000
                
                # Convert to thousands for storage
                min_sal_k = min_sal / 1000
                max_sal_k = max_sal / 1000
                avg_sal_k = (min_sal_k + max_sal_k) / 2
                
                return min_sal_k, max_sal_k, avg_sal_k
            return None, None, None

        salaries = df_out['Salary Estimate'].apply(extract_salary)
        df_out['Salary_Min_K'] = salaries.apply(lambda x: x[0])
        df_out['Salary_Max_K'] = salaries.apply(lambda x: x[1])
        df_out['Salary_Avg_K'] = salaries.apply(lambda x: x[2])
        
        change_log.append("Parsed 'Salary Estimate': Created 'Salary_Min_K', 'Salary_Max_K', 'Salary_Avg_K' (in thousands).")
    else:
        change_log.append("Column 'Salary Estimate' not found. No transformation applied.")

    return df_out, change_log


def extract_job_seniority(df):
    """
    Extracts job seniority level from the 'Job Title' column.
    
    Reduces cardinality by mapping titles to standardized seniority levels:
    'Senior', 'Mid', 'Junior', 'Entry', or 'Unknown'. This helps in analyzing
    salary vs. experience.
    """
    df_out = df.copy()
    change_log = []

    if 'Job Title' in df_out.columns:
        def get_seniority(title):
            if not isinstance(title, str):
                return 'Unknown'
            
            title_lower = title.lower()
            
            if any(word in title_lower for word in ['senior', 'sr.', 'sr', 'lead', 'principal', 'manager', 'director']):
                return 'Senior'
            elif any(word in title_lower for word in ['mid', 'middle', 'experienced']):
                return 'Mid'
            elif any(word in title_lower for word in ['junior', 'jr.', 'jr', 'entry', 'intern', 'associate']):
                return 'Junior'
            elif any(word in title_lower for word in ['entry']):
                return 'Entry'
            else:
                return 'Mid' # Default to Mid if no specific keyword found for generic titles

        df_out['Job_Seniority'] = df_out['Job Title'].apply(get_seniority)
        change_log.append("Extracted 'Job_Seniority': Mapped titles to Senior/Mid/Junior/Entry levels.")
    else:
        change_log.append("Column 'Job Title' not found. No transformation applied.")

    return df_out, change_log


def calculate_company_age(df):
    """
    Converts the 'Founded' year into 'Company_Age'.
    
    Uses 2026 as the reference year (per instructions). Handles missing values
    (represented as -1) by returning None. This transforms a point-in-time date
    into a duration metric which is often more useful for modeling.
    """
    df_out = df.copy()
    change_log = []

    if 'Founded' in df_out.columns:
        def get_age(year):
            if year == -1 or pd.isna(year):
                return None
            return 2026 - year

        df_out['Company_Age'] = df_out['Founded'].apply(get_age)
        change_log.append("Calculated 'Company_Age': Converted 'Founded' year to age (reference year 2026).")
    else:
        change_log.append("Column 'Founded' not found. No transformation applied.")

    return df_out, change_log


def extract_location_state(df):
    """
    Extracts the state/province code from the 'Location' column.
    
    Simplifies location information (e.g., "New York, NY" -> "NY") to reduce
    cardinality and enable regional analysis. Assumes the state is the last
    part of the string after a comma.
    """
    df_out = df.copy()
    change_log = []

    if 'Location' in df_out.columns:
        def get_state(loc):
            if not isinstance(loc, str):
                return None
            parts = loc.split(',')
            if len(parts) > 1:
                return parts[-1].strip()
            return loc.strip()

        df_out['Location_State'] = df_out['Location'].apply(get_state)
        change_log.append("Extracted 'Location_State': Isolated state code from 'Location'.")
    else:
        change_log.append("Column 'Location' not found. No transformation applied.")

    return df_out, change_log


def extract_headquarters_state(df):
    """
    Extracts the state/province code from the 'Headquarters' column.
    
    Similar to location extraction, this isolates the state to facilitate
    comparison between job location and company headquarters.
    """
    df_out = df.copy()
    change_log = []

    if 'Headquarters' in df_out.columns:
        def get_state(hq):
            if not isinstance(hq, str) or hq == '-1':
                return None
            parts = hq.split(',')
            if len(parts) > 1:
                return parts[-1].strip()
            return hq.strip()

        df_out['Headquarters_State'] = df_out['Headquarters'].apply(get_state)
        change_log.append("Extracted 'Headquarters_State': Isolated state code from 'Headquarters'.")
    else:
        change_log.append("Column 'Headquarters' not found. No transformation applied.")

    return df_out, change_log


def compare_location_hq(df):
    """
    Creates a boolean column indicating if the job location matches the headquarters location.
    
    This comparison can be useful for analyzing remote work patterns or regional
    hiring preferences. Returns True if states match, False otherwise.
    """
    df_out = df.copy()
    change_log = []

    if 'Location_State' in df_out.columns and 'Headquarters_State' in df_out.columns:
        df_out['Is_Location_HQ_Same'] = (df_out['Location_State'] == df_out['Headquarters_State'])
        # Handle cases where one might be None
        df_out['Is_Location_HQ_Same'] = df_out['Is_Location_HQ_Same'].fillna(False)
        change_log.append("Created 'Is_Location_HQ_Same': Boolean comparison of Location and HQ states.")
    else:
        change_log.append("Required columns 'Location_State' or 'Headquarters_State' not found. Run extraction functions first.")

    return df_out, change_log


def extract_key_skills(df):
    """
    Performs one-hot encoding for key data science skills found in 'Job Description'.
    
    Searches for common tools and technologies (Python, SQL, Excel, Tableau, AWS, etc.)
    and creates binary indicator columns (1 if present, 0 if not). This converts
    unstructured text into structured features suitable for ML models.
    """
    df_out = df.copy()
    change_log = []

    if 'Job Description' in df_out.columns:
        # Define key skills to search for (lowercase)
        skills = [
            'python', 'r ', 'sql', 'excel', 'tableau', 'power bi', 'powerbi', 
            'spark', 'hadoop', 'aws', 'azure', 'java', 'scala', 'sas', 
            'machine learning', 'deep learning', 'nlp', 'tensorflow', 'pytorch'
        ]
        
        desc_lower = df_out['Job Description'].str.lower()
        
        for skill in skills:
            # Create column name, replacing spaces with underscores
            col_name = f"skill_{skill.replace(' ', '_').replace('-', '_')}"
            # Check if skill string exists in description
            df_out[col_name] = desc_lower.str.contains(skill, na=False).astype(int)
            
        change_log.append(f"Extracted Key Skills: Created one-hot encoded columns for {len(skills)} skills.")
    else:
        change_log.append("Column 'Job Description' not found. No transformation applied.")

    return df_out, change_log


def handle_missing_numeric_values(df):
    """
    Replaces missing value indicators (-1) with NaN in numeric columns.
    
    The analysis identified -1 as a missing indicator for 'Rating' and 'Founded'.
    This function standardizes these to NaN for correct statistical handling.
    """
    df_out = df.copy()
    change_log = []

    numeric_cols_with_neg_ones = ['Rating', 'Founded']
    
    for col in numeric_cols_with_neg_ones:
        if col in df_out.columns:
            count_before = df_out[col].count()
            df_out[col] = df_out[col].replace(-1, None)
            count_after = df_out[col].count()
            diff = count_before - count_after
            change_log.append(f"Handled missing values in '{col}': Replaced {diff} instances of -1 with NaN.")
            
    return df_out, change_log

===== REASONING =====
#Use this file to store the function name for each piece of generated code,
#and the reasoning for why each piece of transformative code was written/produced the way it was

# === APPEND REASONING BELOW ===

1. clean_company_name
   - Reasoning: The data analysis showed 92.6% of Company Name values contained embedded newlines (e.g., "Company\\n3.5"). This is a data quality issue where the rating was concatenated with the name. Splitting on the newline isolates the actual company name, making the data cleaner for grouping or joining operations.

2. clean_job_description
   - Reasoning: 94% of Job Descriptions contained newlines. While newlines are useful for display, they are problematic for text analysis, regex matching, and skill extraction. Replacing them with spaces standardizes the text format.

3. parse_salary_estimate
   - Reasoning: The Salary Estimate column was a string range (e.g., "$80K-$120K"). This cannot be used for numerical analysis. I extracted the min and max values, handled the 'K' suffix (interpreting it as thousands per instructions), and calculated an average. I stored these in thousands (e.g., 80 instead of 80000) to follow the ML dataset standards provided in the helper registry.

4. extract_job_seniority
   - Reasoning: The Job Title column had high cardinality (172 unique values). To reduce this and add analytical value, I mapped titles to seniority levels (Senior, Mid, Junior, Entry) based on keyword detection. This allows for easier analysis of salary vs. experience level.

5. calculate_company_age
   - Reasoning: The helper registry specified converting founding years to entity age. Using 2026 as the reference year (as per the example in the instructions), I calculated the age of the company. This duration metric is often more predictive in models than the raw year.

6. extract_location_state
   - Reasoning: The helper registry requested simplifying location information (e.g., "New York, NY" -> "NY"). I extracted the state code to reduce cardinality and enable regional analysis (e.g., cost of living comparisons by state).

7. extract_headquarters_state
   - Reasoning: Similar to location extraction, this prepares the Headquarters data for comparison. Isolating the state allows for a direct comparison with the job location state.

8. compare_location_hq
   - Reasoning: The helper registry suggested using boolean logic to determine if two elements are the same. Comparing Location and Headquarters state can indicate if a role is likely at the company's main campus or a satellite office, which is a useful feature for analysis.

9. extract_key_skills
   - Reasoning: The helper registry requested extracting binary indicators for key skills. I selected a list of common data science tools (Python, SQL, AWS, etc.) and created one-hot encoded columns. This transforms unstructured text into structured features ready for machine learning.

10. handle_missing_numeric_values
    - Reasoning: The analysis identified -1 as a missing value indicator for 'Rating' and 'Founded'. Standard practice is to represent missing data as NaN so that pandas functions (like mean() or median()) ignore them rather than treating -1 as a valid value.