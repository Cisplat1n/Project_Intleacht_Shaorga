===== FUNC_TEST_SUITE =====
#This txt serves as the template that will store the suggested code: 
# DO NOT MODIFY THE TEXT IN THIS DOCUMENT OTHER THAN TO APPEND CODE TO IT
#This test suite should have a logical flow and spaces + comments/docstrings to explain each piece of code

#Rules:
#You MUST NOT modify, delete, or reorder any existing text.
#You MAY ONLY append new content to the end of the document.
#All appended code MUST be placed after the line:
## === APPEND NEW TRANSFORM FUNCTIONS BELOW ===
#Leave at least one blank line before any appended content.
#Preserve all existing comments and spacing exactly as-is.

#Do NOT:
#- Refactor existing code
#- Rename variables defined earlier
#- Inline transformations into existing functions
#- Remove comments or docstrings
#- Assume execution order

#Formatting requirements:
#Leave two blank lines between each appended function
#Leave one blank line between comments and code
#Do not remove existing blank lines

#Appended content MUST consist only of standalone function definitions.
#Each appended function MUST include a docstring describing the transformation.
#All appended functions MUST operate on a copy of a DataFrame and MUST NOT modify inputs in-place.
#Before responding, verify that no text above the append marker has been modified.

# === APPEND NEW TRANSFORM FUNCTIONS BELOW ===


def clean_company_name(df):
    """
    Cleans the 'Company Name' column by removing embedded newline characters.
    
    The data analysis indicated that 92.6% of values in 'Company Name' contain newlines,
    often separating the company name from a rating (e.g., "Company Name\\n3.5").
    This transformation splits the string on the newline and retains only the first part,
    effectively isolating the company name.
    
    Args:
        df (pd.DataFrame): Input dataframe.
        
    Returns:
        pd.DataFrame: Dataframe with cleaned 'Company Name'.
        list: Change log describing the operation.
    """
    df_out = df.copy()
    change_log = []
    
    if 'Company Name' in df_out.columns:
        original_count = df_out['Company Name'].count()
        # Split by newline and take the first element
        df_out['Company Name'] = df_out['Company Name'].apply(
            lambda x: x.split('\n')[0] if isinstance(x, str) else x
        )
        new_count = df_out['Company Name'].count()
        change_log.append({
            'action': 'clean_company_name',
            'description': 'Removed embedded newlines from Company Name',
            'affected_rows': original_count
        })
    else:
        change_log.append({
            'action': 'clean_company_name',
            'description': 'Column Company Name not found',
            'status': 'skipped'
        })
        
    return df_out, change_log


def clean_job_description(df):
    """
    Cleans the 'Job Description' column by replacing embedded newlines with spaces.
    
    The analysis identified that 94.0% of job descriptions contain newlines. 
    While these newlines might structure the text for human reading, they can 
    interfere with text processing, regex matching, and NLP tasks. This function
    standardizes the text into a single line per entry.
    
    Args:
        df (pd.DataFrame): Input dataframe.
        
    Returns:
        pd.DataFrame: Dataframe with cleaned 'Job Description'.
        list: Change log describing the operation.
    """
    df_out = df.copy()
    change_log = []
    
    if 'Job Description' in df_out.columns:
        original_count = df_out['Job Description'].count()
        # Replace newlines with a space to preserve word separation
        df_out['Job Description'] = df_out['Job Description'].str.replace('\n', ' ', regex=False)
        new_count = df_out['Job Description'].count()
        change_log.append({
            'action': 'clean_job_description',
            'description': 'Replaced newlines with spaces in Job Description',
            'affected_rows': original_count
        })
    else:
        change_log.append({
            'action': 'clean_job_description',
            'description': ''Column Job Description not found',
            'status': 'skipped'
        })
        
    return df_out, change_log


def parse_salary_estimate(df):
    """
    Parses the 'Salary Estimate' column to extract minimum and maximum salary figures.
    
    The 'Salary Estimate' column contains string ranges like '$80K-$120K' (Glassdoor est.).
    This function extracts the numeric values, handles the 'K' (thousands) suffix,
    and creates two new numeric columns: 'Salary_Min' and 'Salary_Max'.
    Values are stored as integers representing thousands (e.g., 80 for $80,000).
    
    Args:
        df (pd.DataFrame): Input dataframe.
        
    Returns:
        pd.DataFrame: Dataframe with new 'Salary_Min' and 'Salary_Max' columns.
        list: Change log describing the operation.
    """
    import re
    import numpy as np
    
    df_out = df.copy()
    change_log =
    []
    
    if 'Salary Estimate' in df_out.columns:
        # Regex to find patterns like $80K-$120K
        # Handles potential variations in spacing or currency symbols
        pattern = r'\$?(\d+)K?\s*-\s*\$?(\d+)K?'
        
        def extract_salary_range(s):
            if isinstance(s, str):
                match = re.search(pattern, s, re.IGNORECASE)
                if match:
                    min_sal = int(match.group(1))
                    max_sal = int(match.group(2))
                    return min_sal, max_sal
            return np.nan, np.nan

        # Apply extraction
        salaries = df_out['Salary Estimate'].apply(extract_salary_range)
        df_out['Salary_Min'] = [x[0] for x in salaries]
        df_out['Salary_Max'] = [x[1] for x in salaries]
        
        parsed_count = df_out[['Salary_Min', 'Salary_Max']].notna().all(axis=1).sum()
        change_log.append({
            'action': 'parse_salary_estimate',
            'description': 'Parsed Salary Estimate into Salary_Min and Salary_Max (in thousands)',
            'affected_rows': parsed_count
        })
    else:
        change_log.append({
            'action': 'parse_salary_estimate',
            'description': 'Column Salary Estimate not found',
            'status': 'skipped'
        })
        
    return df_out, change_log


def handle_missing_rating(df):
    """
    Handles missing values in the 'Rating' column.
    
    The analysis identified -1.0 as a missing value indicator for 'Rating'.
    This function replaces -1.0 with NaN (Not a Number) to properly represent
    missing data for numerical operations.
    
    Args:
        df (pd.DataFrame): Input dataframe.
        
    Returns:
        pd.DataFrame: Dataframe with 'Rating' cleaned.
        list: Change log describing the operation.
    """
    import numpy as np
    
    df_out = df.copy()
    change_log = []
    
    if 'Rating' in df_out.columns:
        missing_count = (df_out['Rating'] == -1.0).sum()
        df_out['Rating'] = df_out['Rating'].replace(-1.0, np.nan)
        change_log.append({
            'action': 'handle_missing_rating',
            'description': 'Replaced -1.0 with NaN in Rating column',
            'affected_rows': int(missing_count)
        })
    else:
        change_log.append({
            'action': 'handle_missing_rating',
            'description': 'Column Rating not found',
            'status': 'skipped'
        })
        
    return df_out, change_log


def calculate_company_age(df):
    """
    Calculates the age of the company based on the 'Founded' year.
    
    The 'Founded' column contains -1 for missing values. This function replaces
    -1 with NaN and calculates 'Company_Age' by subtracting the founding year
    from the current year (assumed 2024 based on data context, max year 2019).
    Age is often more predictive for models than the raw year.
    
    Args:
        df (pd.DataFrame): Input dataframe.
        
    Returns:
        pd.DataFrame: Dataframe with new 'Company_Age' column.
        list: Change log describing the operation.
    """
    import numpy as np
    
    df_out = df.copy()
    change_log = []
    
    if 'Founded' in df_out.columns:
        # Replace -1 with NaN
        missing_count = (df_out['Founded'] == -1).sum()
        df_out['Founded'] = df_out['Founded'].replace(-1, np.nan)
        
        # Calculate Age (Using 2024 as reference year)
        current_year = 2024
        df_out['Company_Age'] = current_year - df_out['Founded']
        
        valid_age_count = df_out['Company_Age'].notna().sum()
        change_log.append({
            'action': 'calculate_company_age',
            'description': 'Calculated Company_Age from Founded year (Reference Year: 2024)',
            'affected_rows': int(valid_age_count),
            'missing_values_handled': int(missing_count)
        })
    else:
        change_log.append({
            'action': 'calculate_company_age',
            'description': 'Column Founded not found',
            'status': 'skipped'
        })
        
    return df_out, change_log


def extract_location_state(df):
    """
    Extracts the state code from the 'Location' column.
    
    The 'Location' column follows a "City, State" format (e.g., "New York, NY").
    This function extracts the state abbreviation to simplify location data
    for regional analysis and reduce cardinality.
    
    Args:
        df (pd.DataFrame): Input dataframe.
        
    Returns:
        pd.DataFrame: Dataframe with new 'Location_State' column.
        list: Change log describing the operation.
    """
    df_out = df.copy()
    change_log = []
    
    if 'Location' in df_out.columns:
        # Split by comma and take the second part, stripping whitespace
        df_out['Location_State'] = df_out['Location'].apply(
            lambda x: x.split(',')[-1].strip() if isinstance(x, str) and ',' in x else x
        )
        
        extracted_count = df_out['Location_State'].notna().sum()
        change_log.append({
            'action': 'extract_location_state',
            'description': 'Extracted state code from Location column',
            'affected_rows': int(extracted_count)
        })
    else:
        change_log.append({
            'action': 'extract_location_state',
            'description': ''Column Location not found',
            'status': 'skipped'
        })
        
    return df_out, change_log


def extract_job_seniority(df):
    """
    Extracts job seniority level from the 'Job Title' column.
    
    To reduce the high cardinality (172 unique values) of 'Job Title', this function
    scans for keywords indicating seniority (e.g., Senior, Junior, Lead, Manager).
    It creates a new categorical column 'Job_Seniority'.
    
    Args:
        df (pd.DataFrame): Input dataframe.
        
    Returns:
        pd.DataFrame: Dataframe with new 'Job_Seniority' column.
        list: Change log describing the operation.
    """
    import numpy as np
    
    df_out = df.copy()
    change_log = []
    
    if 'Job Title' in df_out.columns:
        def determine_seniority(title):
            if not isinstance(title, str):
                return 'Unknown'
            title_lower = title.lower()
            
            if any(word in title_lower for word in ['director', 'vp', 'vice president']):
                return 'Executive'
            elif any(word in title_lower for word in ['manager', 'head', 'chief']):
                return 'Manager'
            elif any(word in title_lower for word in ['senior', 'sr.', 'sr', 'lead', 'principal']):
                return 'Senior'
            elif any(word in title_lower for word in ['junior', 'jr.', 'jr', 'associate', 'entry']):
                return 'Junior'
            else:
                return 'Mid/Unknown'
        
        df_out['Job_Seniority'] = df_out['Job Title'].apply(determine_seniority)
        
        categorized_count = df_out['Job_Seniority'].notna().sum()
        change_log.append({
            'action': 'extract_job_seniority',
            'description': 'Extracted seniority level (Executive, Manager, Senior, Junior, Mid/Unknown) from Job Title',
            'affected_rows': int(categorized_count)
        })
    else:
        change_log.append({
            'action': 'extract_job_seniority',
            'description': 'Column Job Title not found',
            'status': 'skipped'
        })
        
    return df_out, change_log


def extract_job_skills(df):
    """
    Performs one-hot encoding for key technical skills found in 'Job Description'.
    
    This function scans the 'Job Description' text for a predefined list of 
    common data/tech skills (e.g., Python, SQL, Excel, AWS). It creates binary
    indicator columns (0 or 1) for each skill, enabling ML models to utilize
    unstructured text data.
    
    Args:
        df (pd.DataFrame): Input dataframe.
        
    Returns:
        pd.DataFrame: Dataframe with binary skill columns.
        list: Change log describing the operation.
    """
    import numpy as np
    
    df_out = df.copy()
    change_log = []
    
    if 'Job Description' in df_out.columns:
        # Define key skills to search for
        # Note: 'r ' includes space to avoid matching words ending in 'r'
        skills = ['python', 'sql', 'r ', 'excel', 'tableau', 'power bi', 'aws', 'spark', 'hadoop', 'java', 'scala', 'sas']
        
        desc_series = df_out['Job Description'].fillna('').str.lower()
        
        for skill in skills:
            col_name = f'Skill_{skill.replace(" ", "_").replace(".", "").strip()}'
            # Check if skill string exists in description
            df_out[col_name] = desc_series.str.contains(skill, regex=False).astype(int)
            
        change_log.append({
            'action': 'extract_job_skills',
            'description': f'Created one-hot encoded columns for {len(skills)} key skills',
            'skills_added': skills
        })
    else:
        change_log.append({
            'action': 'extract_job_skills',
            'description': 'Column Job Description not found',
            'status': 'skipped'
        })
        
    return df_out, change_log


def compare_hq_location(df):
    """
    Creates a boolean feature indicating if the job location matches the company headquarters.
    
    This compares the 'Location' and 'Headquarters' columns. A match (True) might imply
    onsite work at the main campus, while a mismatch (False) could indicate a remote role,
    a satellite office, or data inconsistency.
    
    Args:
        df (pd.DataFrame): Input dataframe.
        
    Returns:
        pd.DataFrame: Dataframe with new 'Is_HQ_Location' column.
        list: Change log describing the operation.
    """
    df_out = df.copy()
    change_log = []
    
    if 'Location' in df_out.columns and 'Headquarters' in df_out.columns:
        # Direct comparison. Handles potential whitespace issues via strip.
        df_out['Is_HQ_Location'] = (
            df_out['Location'].str.strip() == df_out['Headquarters'].str.strip()
        )
        
        # Handle potential NaNs resulting in False (if desired, though usually NaN comparison is False)
        # Here we leave standard pandas comparison behavior (NaN != NaN).
        
        match_count = df_out['Is_HQ_Location'].sum()
        change_log.append({
            'action': 'compare_hq_location',
            'description': 'Compared Location and Headquarters to create Is_HQ_Location boolean flag',
            'true_count': int(match_count)
        })
    else:
        change_log.append({
            'action': 'compare_hq_location',
            'description': 'Columns Location or Headquarters not found',
            'status': 'skipped'
        })
        
    return df_out, change_log

===== REASONING =====
#Use this file to store the function name for each piece of generated code,
#and the reasoning for why each piece of transformative code was written/produced the way it was

# === APPEND REASONING BELOW ===

1. clean_company_name
   - Reasoning: The critical issues analysis flagged 'Company Name' as having embedded newlines in 92.6% of values (e.g., "Company\\n3.5"). This concatenation of name and rating makes the column unusable for grouping or analysis by company. Splitting on the newline and taking the first element restores the clean company name.

2. clean_job_description
   - Reasoning: The 'Job Description' column contained embedded newlines in 94% of entries. While this preserves formatting for human readers, it complicates text processing, regex extraction, and NLP tasks. Replacing newlines with spaces standardizes the text into a single string per row, facilitating subsequent skill extraction.

3. parse_salary_estimate
   - Reasoning: 'Salary Estimate' was identified as a range format (e.g., "$80K-$120K") which is non-numeric. The helper registry specifies storing salary in thousands. This function uses regex to extract the numeric bounds, handles the 'K' suffix (thousands), and creates 'Salary_Min' and 'Salary_Max' columns as integers (e.g., 80, 120), making the data ready for numerical analysis.

4. handle_missing_rating
   - Reasoning: The numeric summary flagged 'Rating' as having -1.0 values, which are likely missing indicators rather than valid ratings (since the range is 0-5). Replacing -1.0 with `np.nan` ensures that statistical calculations (mean, median) are not skewed by these placeholder values.

5. calculate_company_age
   - Reasoning: 'Founded' contained -1 values for missing data. The helper registry suggests converting dates to age/time_since metrics. Calculating 'Company_Age' (Current Year - Founded) is often more predictive for modeling than the raw year. I used 2024 as the reference year given the dataset's max year is 2019.

6. extract_location_state
   - Reasoning: 'Location' had high cardinality (207 unique values). The helper registry advises simplifying location information (e.g., "New York, NY" -> "NY"). Extracting the state code reduces dimensionality while retaining regional information necessary for geographic analysis.

7. extract_job_seniority
   - Reasoning: 'Job Title' had high cardinality (172 unique values). The helper registry suggests extracting seniority attributes. I mapped titles to broader categories (Executive, Manager, Senior, Junior, Mid/Unknown) based on keyword presence. This reduces cardinality and creates a feature useful for salary prediction models.

8. extract_job_skills
   - Reasoning: The helper registry requests binary indicators for key skills found in job descriptions. I selected a list of common data/tech skills (Python, SQL, Excel, etc.) and created one-hot encoded columns. This transforms unstructured text into a format suitable for machine learning algorithms.

9. compare_hq_location
   - Reasoning: The helper registry suggests using boolean logic to determine if two elements are the same. Comparing 'Location' and 'Headquarters' provides insight into whether a position is at the main campus or a satellite/remote office, which can be a significant feature in predicting salary or job type.

===== SUGGESTIONS =====
1. Parse 'Revenue' into Numeric Ranges
   - What: The 'Revenue' column contains text ranges like "$10 to $25 million (USD)" or "Unknown / Non-Applicable".
   - Why: Converting these to numeric values (e.g., extracting the upper bound or midpoint in millions) would allow for correlation analysis between company size/revenue and salary.
   - Risks: "Unknown" values would need to be handled (imputed or flagged). The ranges vary in scale (millions vs billions), requiring careful normalization.

2. Parse 'Size' into Numeric Employee Counts
   - What: The 'Size' column contains ranges like "10000+ employees" or "51 to 200 employees".
   - Why: Similar to revenue, converting this to a numeric midpoint or upper bound allows for quantitative analysis of company size impact.
   - Risks: "Unknown" values exist. The open-ended upper bound ("10000+") requires an arbitrary cutoff or log-transformation strategy.

3. Reduce Cardinality of 'Industry' and 'Sector'
   - What: 'Industry' has 58 unique values and 'Sector' has 23. While Sector is manageable, Industry is high.
   - Why: For ML models, high cardinality categorical features can lead to overfitting.
   - Suggestions: Consider grouping rare industries into an "Other" category, or using frequency encoding (replacing the category name with its occurrence count) instead of one-hot encoding.

4. Analyze 'Competitors' for Market Density
   - What: The 'Competitors' column lists competitor names separated by commas, or "-1" if none.
   - Why: The number of competitors could serve as a proxy for market competition or industry fragmentation.
   - Suggestions: Create a feature 'Competitor_Count' by counting the commas in the string (plus one). This converts a text list into a numeric feature.

5. Impute Missing 'Rating' and 'Founded'
   - What: The transformations replaced -1 with NaN, resulting in missing data.
   - Why: Many ML algorithms cannot handle NaNs.
   - Suggestions: Impute 'Rating' using the median rating of the 'Sector' or 'Industry' (companies in the same sector often have similar ratings). Impute 'Founded' (and thus 'Company_Age') using the median age of the 'Industry'.