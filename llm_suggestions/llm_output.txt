===== FUNC_TEST_SUITE =====
#This txt serves as the template that will store the suggested code: 
# DO NOT MODIFY THE TEXT IN THIS DOCUMENT OTHER THAN TO APPEND CODE TO IT
#This test suite should have a logical flow and spaces + comments/docstrings to explain each piece of code

#Rules:
#You MUST NOT modify, delete, or reorder any existing text.
#You MAY ONLY append new content to the end of the document.
#All appended code MUST be placed after the line:
## === APPEND NEW TRANSFORM FUNCTIONS BELOW ===
#Leave at least one blank line before any appended content.
#Preserve all existing comments and spacing exactly as-is.

#Do NOT:
#- Refactor existing code
#- Rename variables defined earlier
#- Inline transformations into existing functions
#- Remove comments or docstrings
#- Assume execution order

#Formatting requirements:
#Leave two blank lines between each appended function.
#Leave one blank line between comments and code.
#Do not remove existing blank lines.

#Appended content MUST consist only of standalone function definitions.
#Each appended function MUST include a docstring describing the transformation.
#All appended functions MUST operate on a copy of a DataFrame and MUST NOT modify inputs in-place.
#Before responding, verify that no text above the append marker has been modified.

# === APPEND NEW TRANSFORM FUNCTIONS BELOW


def clean_numeric_missing_indicators(df):
    """
    Replaces negative values (-1) in numeric columns with NaN to properly handle missing data.
    
    Targets:
    - Rating: -1.0 indicates missing rating (7.44% of data)
    - Founded: -1.0 indicates missing founding year (17.56% of data)
    
    Args:
        df (pd.DataFrame): Input dataframe
        
    Returns:
        pd.DataFrame: Dataframe with missing indicators replaced by NaN
        list: Log of changes made
    """
    df_out = df.copy()
    change_log = []
    
    # Columns identified with negative missing indicators
    cols_to_clean = ['Rating', 'Founded']
    
    for col in cols_to_clean:
        if col in df_out.columns:
            missing_count = (df_out[col] == -1).sum()
            if missing_count > 0:
                df_out[col] = df_out[col].replace(-1, np.nan)
                change_log.append(f"Replaced {missing_count} instances of -1 with NaN in '{col}'")
    
    return df_out, change_log


def clean_string_missing_indicators(df):
    """
    Replaces specific string values ('-1', 'Unknown') with NaN in categorical columns.
    
    Targets:
    - Headquarters, Size, Type of ownership, Industry, Sector, Revenue, Competitors
    
    Args:
        df (pd.DataFrame): Input dataframe
        
    Returns:
        pd.DataFrame: Dataframe with string missing indicators replaced by NaN
        list: Log of changes made
    """
    df_out = df.copy()
    change_log = []
    
    # Columns identified with string missing indicators
    cols_to_clean = [
        'Headquarters', 'Size', 'Type of ownership', 
        'Industry', 'Sector', 'Revenue', 'Competitors'
    ]
    
    missing_indicators = ['-1', 'Unknown']
    
    for col in cols_to_clean:
        if col in df_out.columns:
            # Count replacements before making changes
            mask = df_out[col].isin(missing_indicators)
            missing_count = mask.sum()
            
            if missing_count > 0:
                df_out.loc[mask, col] = np.nan
                change_log.append(f"Replaced {missing_count} missing indicators with NaN in '{col}'")
    
    return df_out, change_log


def clean_company_name(df):
    """
    Removes embedded rating information from the Company Name column.
    
    Issue: 92.6% of Company Name values contain a newline followed by a rating (e.g., "Company Name\n4.5").
    This function splits the string on the newline and retains only the company name portion.
    
    Args:
        df (pd.DataFrame): Input dataframe
        
    Returns:
        pd.DataFrame: Dataframe with cleaned Company Name column
        list: Log of changes made
    """
    df_out = df.copy()
    change_log = []
    
    if 'Company Name' in df_out.columns:
        # Check if newlines exist
        has_newline = df_out['Company Name'].astype(str).str.contains('\n', na=False)
        count = has_newline.sum()
        
        if count > 0:
            # Split by newline and take the first part
            df_out['Company Name'] = df_out['Company Name'].apply(
                lambda x: x.split('\n')[0] if isinstance(x, str) and '\n' in x else x
            )
            change_log.append(f"Removed embedded ratings from {count} values in 'Company Name'")
    
    return df_out, change_log


def clean_job_description_text(df):
    """
    Removes embedded newline characters from the Job Description column.
    
    Issue: 94% of Job Description values contain newlines, likely due to concatenation 
    of multi-line text fields. This function replaces newlines with spaces to create 
    continuous text.
    
    Args:
        df (pd.DataFrame): Input dataframe
        
    Returns:
        pd.DataFrame: Dataframe with cleaned Job Description column
        list: Log of changes made
    """
    df_out = df.copy()
    change_log = []
    
    if 'Job Description' in df_out.columns:
        # Check for newlines
        has_newline = df_out['Job Description'].astype(str).str.contains(r'[\n\r]', na=False)
        count = has_newline.sum()
        
        if count > 0:
            # Replace newlines and carriage returns with a space
            df_out['Job Description'] = df_out['Job Description'].str.replace(r'[\n\r]+', ' ', regex=True)
            # Clean up potential double spaces created by replacement
            df_out['Job Description'] = df_out['Job Description'].str.replace(r'\s+', ' ', regex=True).str.strip()
            change_log.append(f"Removed newlines from {count} values in 'Job Description'")
    
    return df_out, change_log


def parse_salary_estimate(df):
    """
    Parses the Salary Estimate column to extract minimum and maximum salary values.
    
    Issue: 100% of values are in range format (e.g., "$100K-$150K").
    This function extracts the numeric values, handles the 'K' (thousands) suffix,
    and creates new numeric columns for analysis.
    
    Args:
        df (pd.DataFrame): Input dataframe
        
    Returns:
        pd.DataFrame: Dataframe with new Salary_Min and Salary_Max columns
        list: Log of changes made
    """
    import re
    
    df_out = df.copy()
    change_log = []
    
    if 'Salary Estimate' in df_out.columns:
        # Initialize new columns with NaN
        df_out['Salary_Min'] = np.nan
        df_out['Salary_Max'] = np.nan
        
        def extract_salary_range(salary_str):
            if not isinstance(salary_str, str):
                return np.nan, np.nan
            
            # Find all numbers followed by optional 'K'
            # Pattern handles: $100K-$150K, $100-$150, etc.
            matches = re.findall(r'(\d+)K?', salary_str)
            
            if len(matches) >= 2:
                # Convert to int, multiply by 1000 if 'K' was implied (standard for this dataset context)
                # Since the pattern matches digits, we check if 'K' was present or assume based on context
                # Given the stats show DIGIT(3), it's likely thousands.
                min_sal = int(matches[0])
                max_sal = int(matches[1])
                
                # Check if 'K' is in the string to determine scale
                if 'K' in salary_str.upper():
                    min_sal *= 1000
                    max_sal *= 1000
                
                return min_sal, max_sal
            
            return np.nan, np.nan
        
        # Apply extraction
        extracted_data = df_out['Salary Estimate'].apply(extract_salary_range)
        df_out['Salary_Min'] = [x[0] for x in extracted_data]
        df_out['Salary_Max'] = [x[1] for x in extracted_data]
        
        valid_count = df_out['Salary_Min'].notna().sum()
        change_log.append(f"Parsed {valid_count} salary ranges into 'Salary_Min' and 'Salary_Max' columns")
    
    return df_out, change_log

===== REASONING =====
#Use this file to store the function name for each piece of generated code,
#and the reasoning for why each piece of transformative code was written/produced the way it was

# === APPEND REASONING BELOW ===

Function: clean_numeric_missing_indicators
Reasoning: The numeric summary identified that 'Rating' and 'Founded' contain negative values (-1.0), which are explicitly flagged as 'likely_missing_indicator'. In pandas, -1 is treated as a valid number which would skew statistical analysis (mean, min, etc.). Replacing these with np.nan standardizes missing data handling and allows pandas to ignore them in numerical operations automatically.

Function: clean_string_missing_indicators
Reasoning: The categorical summary identified specific string values like '-1' and 'Unknown' acting as missing indicators in multiple columns (Headquarters, Size, Type of ownership, etc.). Leaving these as strings prevents these columns from being treated as true categorical dimensions or being grouped correctly. Standardizing them to np.nan ensures consistency across the dataset.

Function: clean_company_name
Reasoning: The critical issues analysis flagged 'Company Name' as having embedded newlines (92.6% of values). The structure example "ALPHA(11)-NEWLINE-DIGIT(1)-'.'-DIGIT(1)" indicates the rating is appended to the name. Since 'Rating' is already a separate column, this data is redundant and corrupts the 'Company Name' field. Splitting on the newline and keeping the first part restores the actual company name.

Function: clean_job_description_text
Reasoning: The critical issues analysis flagged 'Job Description' as having embedded newlines (94% of values). While descriptions are naturally long, the newlines here likely result from scraping or concatenation artifacts. Removing them creates a clean string block, which is generally preferred for text processing, tokenization, or simple display, without losing semantic information.

Function: parse_salary_estimate
Reasoning: The critical issues analysis flagged 'Salary Estimate' as a range format (100% of values). The current string format (e.g., "$100K-$150K") is not analytically useful. We cannot calculate average salary, filter by salary range, or perform correlations. Parsing this into two numeric columns ('Salary_Min' and 'Salary_Max') unlocks quantitative analysis. The regex handles the 'K' suffix by multiplying by 1000, converting the values into raw integer figures suitable for math operations.

===== ENHANCEMENT SUGGESTIONS =====

1. **Feature: Company Age Calculation**
   - **What:** Create a new column `Company_Age` by subtracting the `Founded` year from the current year (or a fixed reference year if the data is static).
   - **Why:** The age of a company can be a strong predictor for salary ranges, job stability, and company culture. Startups (young) often pay differently than established corporations (old).
   - **Risks:** Requires handling the missing `Founded` values (currently -1/NaN) carefully to avoid calculating negative ages or errors.

2. **Feature: Seniority Extraction from Job Title**
   - **What:** Parse the `Job Title` column to categorize roles into seniority levels (e.g., 'Senior', 'Lead', 'Manager', 'Junior', 'Entry').
   - **Why:** Job titles are high cardinality (172 unique values). Grouping them by seniority reduces dimensionality and allows for analysis of compensation vs. experience level.
   - **Risks:** Keyword matching can be imperfect (e.g., "Product Manager" vs "Project Manager"). Some titles might not contain explicit seniority keywords.

3. **Feature: State Extraction from Location**
   - **What:** Split the `Location` column (format "City, State") into two separate columns: `City` and `State`.
   - **Why:** Geographic analysis is often done at the state or regional level rather than specific city level. This enables grouping by state to compare cost-of-living adjustments or regional job markets.
   - **Risks:** Some locations might be remote or country-specific without a standard "City, State" format, requiring robust error handling.

4. **Encoding Strategy: One-Hot Encoding for Categorical Variables**
   - **What:** Apply one-hot encoding to low-cardinality categorical columns like `Size`, `Type of ownership`, `Sector`, and `Revenue`.
   - **Why:** Machine learning models require numeric input. One-hot encoding converts these categories into binary columns, allowing the model to interpret the impact of, for example, "Public" vs "Private" ownership on salary.
   - **Risks:** Increases dimensionality significantly if applied to high-cardinality columns like `Industry` or `Location`.

5. **Feature: Average Salary Calculation**
   - **What:** Create a column `Salary_Avg` calculated as `(Salary_Min + Salary_Max) / 2`.
   - **Why:** Simplifies analysis. Instead of filtering by ranges, you can sort or visualize by a single representative salary value per job.
   - **Risks:** Assumes a linear distribution within the range, which might not always be true, but is a standard approximation.